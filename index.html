<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title></title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/dracula.css" id="theme" />
    <link rel="stylesheet" href="plugin/highlight/zenburn.css" />
	<link rel="stylesheet" href="css/layout.css" />
	<link rel="stylesheet" href="plugin/customcontrols/style.css">
	<link rel="stylesheet" href="plugin/chalkboard/style.css">

	<link rel="stylesheet" href="plugin/reveal-pointer/pointer.css" />


    <script defer src="dist/fontawesome/all.min.js"></script>

	<script type="text/javascript">
		var forgetPop = true;
		function onPopState(event) {
			if(forgetPop){
				forgetPop = false;
			} else {
				parent.postMessage(event.target.location.href, "app://obsidian.md");
			}
        }
		window.onpopstate = onPopState;
		window.onmessage = event => {
			if(event.data == "reload"){
				window.document.location.reload();
			}
			forgetPop = true;
		}

		function fitElements(){
			const itemsToFit = document.getElementsByClassName('fitText');
			for (const item in itemsToFit) {
				if (Object.hasOwnProperty.call(itemsToFit, item)) {
					var element = itemsToFit[item];
					fitElement(element,1, 1000);
					element.classList.remove('fitText');
				}
			}
		}

		function fitElement(element, start, end){

			let size = (end + start) / 2;
			element.style.fontSize = `${size}px`;

			if(Math.abs(start - end) < 1){
				while(element.scrollHeight > element.offsetHeight){
					size--;
					element.style.fontSize = `${size}px`;
				}
				return;
			}

			if(element.scrollHeight > element.offsetHeight){
				fitElement(element, start, size);
			} else {
				fitElement(element, size, end);
			}		
		}


		document.onreadystatechange = () => {
			fitElements();
			if (document.readyState === 'complete') {
				if (window.location.href.indexOf("?export") != -1){
					parent.postMessage(event.target.location.href, "app://obsidian.md");
				}
				if (window.location.href.indexOf("print-pdf") != -1){
					let stateCheck = setInterval(() => {
						clearInterval(stateCheck);
						window.print();
					}, 250);
				}
			}
	};


        </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Unsupervised Reinforcement Learning for Fast Novel Task Adaptation 

Methods of unsupervised pre-training for fast multi-task adaptation in Reinforcement Learning (RL)
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Outline

- Intro to RL
- Exploration algorithms
- Fast Task Adaptation Benchmarks: (URLB, ExORL)
- Reward Model
- DreamerMPC
- Future work
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Intro to RL

<img src="attachements/reinforcement-learning-method-1024x692.png" alt="" style="object-fit: scale-down">

Goal is to maximize accumulated reward
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcment Learning
#### Achivements 

<img src="attachements/Atari.png" alt="" style="object-fit: scale-down">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcment Learning
#### Achivements 

<img src="attachements/gettyimages-515343878.webp" alt="" style="object-fit: scale-down">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcment Learning
#### Achivements 

<img src="attachements/chat-gpt-header.jpg" alt="" style="object-fit: scale-down">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcement Learning
#### Limitations

- Cannot reuse models/data from similar environments
	- Unlike foundational models 
        - CV: BYOL
        - NLP: GPT
- Hard optimization - distributional shift during training
- Hard to specify a reward function
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### What we propose?

#### Fast Task Adaptation
- Introduce a novel approach to unsupervised data utilization
- Significant improvement over previous SOTA (up to *1.7x* average improvement)
- Propose a new benchmark that reduced required task data from 100K to 10K steps
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### What we propose?

#### Fast Task Adaptation
(exploration) (URLB100K) (ours 10K)
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Preliminaries and notations
#### Outline
- MDP definitions
- RL objective
- Q-Learning
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Markov Decision Process
#### MDP

- MDP = < S, T, A, R >
- S = state
- T = transition `$T: S\times A \rightarrow S$`
- A = action (real or discrete)
- R = reward `$R: S\times A \rightarrow \mathbb{R}$`
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RL objective

- The goal is to find a policy `$\pi$` that maximizes the expected discounted reward `$\mathcal{R}=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^T\gamma^tr_t\right]$`
-   Additional definitions:
    - `$Q^\pi(s,a)=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^T\gamma^tr_t|s_0=s,a_0=a\right]$`
    - `$V^\pi(s)=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^T\gamma^tr_t|s_0=s\right]$`
-   Using these definitions we can define the optimal policy as: 
    - `$\pi(s)=argmax_a Q^\pi(s,a)$`
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Q-Learning

-   An approach to find the optimal policy is to look at the bellman equation:
    -   `$Q(s_t,a_t) = \mathbb{E}_{s'\sim p(s'|s,a)}[r_{t+1}+\gamma Q(s',a')]$`
-   Instead if learning `$\pi$` directly, we will learn the Q function, where the loss is:
    - `$||{Q_*(s_t,a_t) - r_{t+1}+\gamma Q(s',a')}||^2$`
-   This can be easily calculated from batches of `$(s,a,r,s')$` batches
</div>

<aside class="notes"><ul>
<li>discuss on/off policy considerations<ul>
<li>We are not dependant on the policy that produced these state/actions, all data is useful</li>
</ul>
</li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Deep Deterministic Policy Gradient
#### (DDPG)

-   A variation of Q-Learning which supports continuous actions
-   Introduces an additional policy network `$\pi_\theta$` which is trained to maximize the Q network
-   This is the basic RL algorithm which is used in this work
</div>

<aside class="notes"><ul>
<li>Emphasis on two learned networks, <code>$Q_\phi$</code> and <code>$\pi_\theta$</code></li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcment Learning
#### Exploration vs Exploitation

-   Sparse reward environments is challenging for optimization
-   We can add an auxiliary reward to facilitate exploration
-   Penalize already visited states
<img src="attachements/rl_ir.png" alt="" style="object-fit: scale-down">
</div>

<aside class="notes"><ul>
<li>Problem with sparse  reward is that most trajectories get 0 reward</li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Reinforcment Learning
#### TV problem

<img src="attachements/tvprob.gif" alt="" style="object-fit: scale-down">
</div>

<aside class="notes"><ul>
<li>TV problem:</li>
<li>The agent may encounter a TV producing endless noise and will be motivated to procrastinate and stay watching the TVthe exploration exploitation problem</li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Exploration by Intrinsic Reward
#### ICM review

-   Intrinsic reward construction
-   Predict state representation
	-   Learned from predicting action from consecutive states
-   Inverse model to fight TV problem

<img src="attachements/icm.jpg" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RND review

-   Reward definition: `$||\hat{f}(s_{t+1})-f(s_{t+1})||$`
-   Predict random representation
-   Optimize   such that we minimize the reward
-   This objective is also the intrinsic reward
-   First better then average human performance on ATARI Montezuma game
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RND review
<img src="attachements/rnd.png" alt="" style="width: 500px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RND review

<img src="attachements/rnd_results.png" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RE3 review

-   Penalize states by k-NN density in random representation
-   Optimizes state covarage by entropy maximization
-   State Entropy is esitmated by `$\log(y_i - y_i^{k-NN} + 1)$`
</div>

<aside class="notes"><ul>
<li>tested specifically on augmented image states</li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### RE3 review

<img src="attachements/re3_results.png" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks

- A general approach for fast task adaptation is to incorporate extra task-agnostic data 
- One approach describes a pre-training and fine-tuning process
- Second approach is to utilize an auxiliary exploration data and treat is as an offline RL problem
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks
#### URLB
- pre-train a policy on unsupervised data, using only an exploration reward, for 2M steps
- fine-tune the policy on the actual task, using 100K steps
- The benchmark tries to establish a comparison of algorithms for transferring unsupervised exploration to specific tasks
#### ExORL
- collect unsupervised data, possibly directed by exploration (2-10M steps)
- assuming a simple reward function, re-label the reward to be the target task reward
- Solve the task using only the re-labeled dataset
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks
#### URLB

-   The benchmark tests:
    -   3 environments
    -   4 tasks each
    -   2 observation types for each (pixel and state) 
-   8 exploration algorithms are compared (we focus on top 2)

<img src="attachements/urlb_tasks.png" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks
#### ExORL

- The benchmark works on the same environments and tasks
- Only skips state based observations and works only on pixels
- Same exploration algorithms are used to collect the offline dataset
- A few Offline RL algorithms are also compared
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks
#### URLB setup drawbacks

-   During pre-training, all the collected data is discarded.
	-   We will refer to it as the Unsupervised Exploration Buffer (UEB)
-   At the end of pre-training, the policy is trained to reach the horizon of explored data. (task dependant)
#### ExORL drawbacks
- Assumes a simple reward function, which allows data re-labeling
	- Not realistic in numerous applications
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Fast Task Adaptation Benchmarks
#### Why not both?
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Exploration Buffer + Reward Model

-   We propose to recycle the exploration data using a RM `$R(s,a)$`.
-   During fine-tuning:
    -   While collecting task data, we sample a batch to train the model:
        -   sampled online data: train the RM.
        -   sampled UEB: infer the task reward using the RM
    -   Data is collected only if we sample online data.
</div>

<aside class="notes"><ul>
<li>Instead of transferring the policy, we transfer the data - pre-populate the replay buffer (Similar approach with known reward function)</li>
<li>During training, we learn a reward model <code>$R(s,a)$</code> and apply it on data from the UEB</li>
<li>Further improve efficiency by collecting data only after sampling from the online replay buffer</li>
<li>Testing additional approach</li>
</ul>
</aside></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="has-light-background drop" data-background-color="#f0f0f0" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Exploration Buffer + Reward Model
#### Setup

<split even>
<img src="attachements/baseline_diag.png" alt="" style="width: 350px; object-fit: fill">

<img src="attachements/buffer_aug_diag_v2.png" alt="" style="width: 350px; object-fit: fill">

</split>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Exploration Buffer + Reward Model
#### Pixels

<img src="attachements/urlb_buffered_rm_pixels_bars.png" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### World Models

-   Another opportunity to leverage the UEB:
    -   Train the world model using the UEB
    -   During fine-tuning:
        -   Continue WM fine-tuning, with the correct reward
        -   Train a policy using DreamerV3
-   Was later demonstrated in another work
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### World Models - Dreamer
#### Dreamer

<img src="attachements/dreamerv3.png" alt="" style="width: 1000px; object-fit: fill">
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Expert Demonstrations
#### Setup
<img src="attachements/expert-clipart-man_clipart2.png" alt="" style="width: 100px; object-fit: fill">


-   We want to further remove reliance on online data acquisition.
-   We Propose to use few expert demonstrations 
-   given 4 expert demos, use 10K additional online data
    -   (Instead of the 100K of URLB)
-   We test this setup using the methods described here
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Expert Demonstrations
#### Experiments

-   Reward Model
    -   Keep UEB and use the RM when sampling it
    -   finetune the exploration policy 
-   DreamerMPC (MURLB)
    -   current SOTA on URLB using World model
    -   during pre-training, train the world model
    -   fine-tune the policy with the world model on the task
-   DreamerMPC+
	-  Based on MURLB
    -   Use the UEB during finetuning
    -   optionally initialize finetuing with 4 expert demos
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Expert Demonstrations
#### 100K vs Reward Model

-   We evaluate at 10K online steps VS 100K on URLB baseline

| Task/Method | URLB 100K | Reward Model | Reward Model+Expert |
| ----------- | --------- | ------------ | ------------------- |
| Walker Walk | 141       | 197          | 375                 |
| Walker Run  | 41        | 57           | 106                 |
| Walker Flip | 81        | 210          | 285                 |
| Quad Walk   | 151       | 90           | 402                 |
| Quad Run    | 156       | 103          | 412                 |
| Quadr Jump  | 278       | 122          | 630                 |
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Expert Demonstrations
#### URLB SOTA (DreamerMPC) vs DreamerMPC+

-   We evaluate at 10K online steps (Average to 2 seeds)

| Task/Method        | DreamerMPC | DreamerMPC+ | DreamerMPC+Expert |
| ------------------ | ---------- | ----------- | ----------------- |
| Walker Walk        | 630        | 896         | *918*             |
| Walker Run         | 328        | *594*       | 585               |
| Walker Flip        | 607        | 841         | *889*             |
| Quadruped Walk     | 339        | 401         | *540*             |
| Quadruped Run      | 200        | 324         | *446*             |
| Quadruped Jump     | 449        | 539         | *915*             |
| Normalized Average | 1.0        | 1.4         | *1.68*            |
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 1024px; width: 768px; min-height: 1024px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

# Thank You!
</div></script></section><section  data-markdown><script type="text/template">
</script></section></div>
    </div>

    <script src="dist/reveal.js"></script>

    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/zoom/zoom.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/math/math.js"></script>
	<script src="plugin/mermaid/mermaid.js"></script>
	<script src="plugin/chart/chart.min.js"></script>
	<script src="plugin/chart/plugin.js"></script>
	<script src="plugin/customcontrols/plugin.js"></script>
	<script src="plugin/chalkboard/plugin.js"></script>
	<script src="plugin/reveal-pointer/pointer.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

	  function isLight(color) {
		let hex = color.replace('#', '');

		// convert #fff => #ffffff
		if(hex.length == 3){
			hex = `${hex[0]}${hex[0]}${hex[1]}${hex[1]}${hex[2]}${hex[2]}`;
		}

		const c_r = parseInt(hex.substr(0, 2), 16);
		const c_g = parseInt(hex.substr(2, 2), 16);
		const c_b = parseInt(hex.substr(4, 2), 16);
		const brightness = ((c_r * 299) + (c_g * 587) + (c_b * 114)) / 1000;
		return brightness > 155;
	}

	var bgColor = getComputedStyle(document.documentElement).getPropertyValue('--r-background-color').trim();
	var isLight = isLight(bgColor);

	if(isLight){
		document.body.classList.add('has-light-background');
	} else {
		document.body.classList.add('has-dark-background');
	}

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath.MathJax3,
		  RevealMermaid,
		  RevealChart,
		  RevealCustomControls,
	      RevealPointer,
		  RevealChalkboard, 
        ],


    	allottedTime: 120 * 1000,

		mathjax3: {
			mathjax: 'plugin/math/mathjax/tex-mml-chtml.js',
		},
		markdown: {
		  gfm: true,
		  mangle: true,
		  pedantic: false,
		  smartLists: false,
		  smartypants: false,
		},

		mermaid: {
			theme: isLight ? 'default' : 'dark',
		},

		customcontrols: {
			controls: [
				{ icon: '<i class="fa fa-pen-square"></i>',
				title: 'Toggle chalkboard (B)',
				action: 'RevealChalkboard.toggleChalkboard();'
				},
				{ icon: '<i class="fa fa-pen"></i>',
				title: 'Toggle notes canvas (C)',
				action: 'RevealChalkboard.toggleNotesCanvas();'
				},
			]
		},
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"width":768,"height":1024,"margin":0.04,"controls":true,"progress":true,"slideNumber":true,"transition":"slide","transitionSpeed":"default"}, queryOptions);
    </script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>

  <!-- created with Advanced Slides -->
</html>
